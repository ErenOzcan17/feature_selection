{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Bu kodda veri tabanımdaki verileri speakers ve sentences değişkenlerine alıyorum",
   "id": "2c55ac8b36143999"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import sqlite3\n",
    "import pickle\n",
    "\n",
    "class Speaker:\n",
    "    _id_counter = 1  # Static counter for Speaker IDs\n",
    "\n",
    "    def __init__(self, client_id):\n",
    "        self.id = Speaker._id_counter  # Assign auto-generated ID\n",
    "        Speaker._id_counter += 1  # Increment counter for next speaker\n",
    "        self.client_id = client_id\n",
    "        self.sentences = []\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        self.sentences.append(sentence)\n",
    "\n",
    "class Sentence:\n",
    "    _id_counter = 1  # Static counter for Sentence IDs\n",
    "\n",
    "    def __init__(self, speaker, path, mfcc1, mfcc2, mfcc3, mfcc4, mfcc5, mfcc6,\n",
    "                 mfcc7, mfcc8, mfcc9, mfcc10, mfcc11, mfcc12, mfcc13,\n",
    "                 chroma1, chroma2, chroma3, chroma4, chroma5, chroma6,\n",
    "                 chroma7, chroma8, chroma9, chroma10, chroma11, chroma12,\n",
    "                 spectral_centroid, spectral_rolloff, zero_crossing_rate,\n",
    "                 spectral_bandwidth, rms_energy, tempo, sentence_id, sentence):\n",
    "        self.id = Sentence._id_counter  # Assign auto-generated ID\n",
    "        Sentence._id_counter += 1  # Increment counter for next sentence\n",
    "        self.speaker = speaker\n",
    "        self.path = path\n",
    "        self.mfcc1 = mfcc1\n",
    "        self.mfcc2 = mfcc2\n",
    "        self.mfcc3 = mfcc3\n",
    "        self.mfcc4 = mfcc4\n",
    "        self.mfcc5 = mfcc5\n",
    "        self.mfcc6 = mfcc6\n",
    "        self.mfcc7 = mfcc7\n",
    "        self.mfcc8 = mfcc8\n",
    "        self.mfcc9 = mfcc9\n",
    "        self.mfcc10 = mfcc10\n",
    "        self.mfcc11 = mfcc11\n",
    "        self.mfcc12 = mfcc12\n",
    "        self.mfcc13 = mfcc13\n",
    "        self.chroma1 = chroma1\n",
    "        self.chroma2 = chroma2\n",
    "        self.chroma3 = chroma3\n",
    "        self.chroma4 = chroma4\n",
    "        self.chroma5 = chroma5\n",
    "        self.chroma6 = chroma6\n",
    "        self.chroma7 = chroma7\n",
    "        self.chroma8 = chroma8\n",
    "        self.chroma9 = chroma9\n",
    "        self.chroma10 = chroma10\n",
    "        self.chroma11 = chroma11\n",
    "        self.chroma12 = chroma12\n",
    "        self.spectral_centroid = spectral_centroid\n",
    "        self.spectral_rolloff = spectral_rolloff\n",
    "        self.zero_crossing_rate = zero_crossing_rate\n",
    "        self.spectral_bandwidth = spectral_bandwidth\n",
    "        self.rms_energy = rms_energy\n",
    "        self.tempo = tempo\n",
    "        self.sentence_id = sentence_id\n",
    "        self.sentence = sentence\n",
    "\n",
    "conn = sqlite3.connect('db.sqlite3')\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create a dictionary to store Speaker objects, using speaker's id as the key\n",
    "speakers = {}\n",
    "sentences = []\n",
    "client_ids_id_conn_dict = {}\n",
    "\n",
    "# Fetch data from the database\n",
    "cur.execute(\"SELECT * FROM sentence_data\")\n",
    "\n",
    "for row in cur.fetchall():\n",
    "    client_id = row[0]\n",
    "    if client_id not in client_ids_id_conn_dict.keys():\n",
    "        speaker = Speaker(client_id)\n",
    "        client_ids_id_conn_dict[client_id] = speaker.id\n",
    "        speakers[speaker.id] = speaker\n",
    "    else:\n",
    "        speaker_id = client_ids_id_conn_dict[client_id]\n",
    "        speaker = speakers[speaker_id]\n",
    "\n",
    "    sentence = Sentence(\n",
    "    speaker=speaker,\n",
    "    path=row[1],\n",
    "    mfcc1=pickle.loads(row[2]),\n",
    "    mfcc2=pickle.loads(row[3]),\n",
    "    mfcc3=pickle.loads(row[4]),\n",
    "    mfcc4=pickle.loads(row[5]),\n",
    "    mfcc5=pickle.loads(row[6]),\n",
    "    mfcc6=pickle.loads(row[7]),\n",
    "    mfcc7=pickle.loads(row[8]),\n",
    "    mfcc8=pickle.loads(row[9]),\n",
    "    mfcc9=pickle.loads(row[10]),\n",
    "    mfcc10=pickle.loads(row[11]),\n",
    "    mfcc11=pickle.loads(row[12]),\n",
    "    mfcc12=pickle.loads(row[13]),\n",
    "    mfcc13=pickle.loads(row[14]),\n",
    "    chroma1=pickle.loads(row[15]),\n",
    "    chroma2=pickle.loads(row[16]),\n",
    "    chroma3=pickle.loads(row[17]),\n",
    "    chroma4=pickle.loads(row[18]),\n",
    "    chroma5=pickle.loads(row[19]),\n",
    "    chroma6=pickle.loads(row[20]),\n",
    "    chroma7=pickle.loads(row[21]),\n",
    "    chroma8=pickle.loads(row[22]),\n",
    "    chroma9=pickle.loads(row[23]),\n",
    "    chroma10=pickle.loads(row[24]),\n",
    "    chroma11=pickle.loads(row[25]),\n",
    "    chroma12=pickle.loads(row[26]),\n",
    "    spectral_centroid=pickle.loads(row[27]),\n",
    "    spectral_rolloff=pickle.loads(row[28]),\n",
    "    zero_crossing_rate=pickle.loads(row[29]),\n",
    "    spectral_bandwidth=pickle.loads(row[30]),\n",
    "    rms_energy=pickle.loads(row[31]),\n",
    "    tempo=row[32],\n",
    "    sentence_id=row[33],\n",
    "    sentence=row[34]\n",
    ")\n",
    "\n",
    "    sentences.append(sentence)\n",
    "    speaker.add_sentence(sentence)\n",
    "\n",
    "conn.close()\n",
    "del cur\n",
    "del conn\n",
    "print(f\"konuşmacı sayısı: {len(speakers)}\")\n",
    "print(f\"konuşma sayısı: {len(sentences)}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Featureları sözlük ve liste olarak topluyorum",
   "id": "296a32910ce183d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Extract features for correlation analysis\n",
    "feature_rows = []\n",
    "feature_dicts = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    feature_dict = {\n",
    "        \"mfcc1\": np.mean(sentence.mfcc1),\n",
    "        \"mfcc2\": np.mean(sentence.mfcc2),\n",
    "        \"mfcc3\": np.mean(sentence.mfcc3),\n",
    "        \"mfcc4\": np.mean(sentence.mfcc4),\n",
    "        \"mfcc5\": np.mean(sentence.mfcc5),\n",
    "        \"mfcc6\": np.mean(sentence.mfcc6),\n",
    "        \"mfcc7\": np.mean(sentence.mfcc7),\n",
    "        \"mfcc8\": np.mean(sentence.mfcc8),\n",
    "        \"mfcc9\": np.mean(sentence.mfcc9),\n",
    "        \"mfcc10\": np.mean(sentence.mfcc10),\n",
    "        \"mfcc11\": np.mean(sentence.mfcc11),\n",
    "        \"mfcc12\": np.mean(sentence.mfcc12),\n",
    "        \"mfcc13\": np.mean(sentence.mfcc13),\n",
    "        \"chroma1\": np.mean(sentence.chroma1),\n",
    "        \"chroma2\": np.mean(sentence.chroma2),\n",
    "        \"chroma3\": np.mean(sentence.chroma3),\n",
    "        \"chroma4\": np.mean(sentence.chroma4),\n",
    "        \"chroma5\": np.mean(sentence.chroma5),\n",
    "        \"chroma6\": np.mean(sentence.chroma6),\n",
    "        \"chroma7\": np.mean(sentence.chroma7),\n",
    "        \"chroma8\": np.mean(sentence.chroma8),\n",
    "        \"chroma9\": np.mean(sentence.chroma9),\n",
    "        \"chroma10\": np.mean(sentence.chroma10),\n",
    "        \"chroma11\": np.mean(sentence.chroma11),\n",
    "        \"chroma12\": np.mean(sentence.chroma12),\n",
    "        \"spectral_centroid\": np.mean(sentence.spectral_centroid),\n",
    "        \"spectral_rolloff\": np.mean(sentence.spectral_rolloff),\n",
    "        \"zero_crossing_rate\": np.mean(sentence.zero_crossing_rate),\n",
    "        \"spectral_bandwidth\": np.mean(sentence.spectral_bandwidth),\n",
    "        \"rms_energy\": np.mean(sentence.rms_energy)\n",
    "    }\n",
    "\n",
    "    feature_row = [\n",
    "        np.mean(sentence.mfcc1),\n",
    "        np.mean(sentence.mfcc2),\n",
    "        np.mean(sentence.mfcc3),\n",
    "        np.mean(sentence.mfcc4),\n",
    "        np.mean(sentence.mfcc5),\n",
    "        np.mean(sentence.mfcc6),\n",
    "        np.mean(sentence.mfcc7),\n",
    "        np.mean(sentence.mfcc8),\n",
    "        np.mean(sentence.mfcc9),\n",
    "        np.mean(sentence.mfcc10),\n",
    "        np.mean(sentence.mfcc11),\n",
    "        np.mean(sentence.mfcc12),\n",
    "        np.mean(sentence.mfcc13),\n",
    "        np.mean(sentence.chroma1),\n",
    "        np.mean(sentence.chroma2),\n",
    "        np.mean(sentence.chroma3),\n",
    "        np.mean(sentence.chroma4),\n",
    "        np.mean(sentence.chroma5),\n",
    "        np.mean(sentence.chroma6),\n",
    "        np.mean(sentence.chroma7),\n",
    "        np.mean(sentence.chroma8),\n",
    "        np.mean(sentence.chroma9),\n",
    "        np.mean(sentence.chroma10),\n",
    "        np.mean(sentence.chroma11),\n",
    "        np.mean(sentence.chroma12),\n",
    "        np.mean(sentence.spectral_centroid),\n",
    "        np.mean(sentence.spectral_rolloff),\n",
    "        np.mean(sentence.zero_crossing_rate),\n",
    "        np.mean(sentence.spectral_bandwidth),\n",
    "        np.mean(sentence.rms_energy)\n",
    "    ]\n",
    "\n",
    "    feature_rows.append(feature_row)\n",
    "    feature_dicts.append(feature_dict)\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "columns = [\n",
    "    'mfcc1', 'mfcc2', 'mfcc3', 'mfcc4', 'mfcc5', 'mfcc6', 'mfcc7', 'mfcc8', 'mfcc9', 'mfcc10',\n",
    "    'mfcc11', 'mfcc12', 'mfcc13', 'chroma1', 'chroma2', 'chroma3', 'chroma4', 'chroma5',\n",
    "    'chroma6', 'chroma7', 'chroma8', 'chroma9', 'chroma10', 'chroma11', 'chroma12',\n",
    "    'spectral_centroid', 'spectral_rolloff', 'zero_crossing_rate', 'spectral_bandwidth',\n",
    "    'rms_energy'\n",
    "]"
   ],
   "id": "13d3a3af855a2297",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Feature selections:\n",
    "\n",
    "1. Corelation Analysis:"
   ],
   "id": "a5f0409d993c57a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(feature_rows, columns=columns)\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Write the correlation matrix to a text file\n",
    "with open(\"correlation_matrix.txt\", \"w\") as f:\n",
    "    f.write(correlation_matrix.to_string())\n",
    "\n",
    "print(\"Correlation matrix has been written to 'correlation_matrix.txt'\")"
   ],
   "id": "b949163b36c6bc85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. Random Forest Classifier:",
   "id": "cbd832dbd967f81e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T09:55:16.417158Z",
     "start_time": "2025-01-27T09:55:16.230224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Prepare features and labels\n",
    "X = np.array(feature_rows)\n",
    "y = [sentence.speaker.id for sentence in sentences]\n",
    "\n",
    "# Train RandomForest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Feature importances\n",
    "importance = rf.feature_importances_\n",
    "\n",
    "# Sort by importance\n",
    "important_features = np.argsort(importance)[::-1]\n",
    "# Write the important features and their importance to a text file\n",
    "with open(\"RF_feature_selection\", \"w\") as f:\n",
    "    for important_feature in important_features:\n",
    "        f.write(f\"{columns[important_feature]}: {importance[important_feature]}\\n\")\n",
    "\n",
    "print(\"Important features have been written to 'RF_feature_selection.txt'.\")\n",
    "\n"
   ],
   "id": "eff231f4fead01d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important features have been written to 'important_features.txt'\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3. PCA (Principal Component Analysis):\n",
    "\n",
    "Verinin boyutu, PCA (Principal Component Analysis) yöntemiyle, verinin temel bileşenleri (principal components) kullanılarak indirgenir. Bu işlem, verinin en fazla varyans gösteren yönlerini bulmayı amaçlar. PCA, orijinal verinin doğrusal kombinasyonlarını oluşturarak boyutları azaltır, ancak verinin önemli özelliklerini mümkün olduğunca korur.\n",
    "\n",
    "Boyut İndirgeme Nasıl Çalışır?\n",
    "Varyans ve Temel Bileşenler (Principal Components):\n",
    "\n",
    "PCA, verinin en fazla varyansı gösterdiği yönleri (temel bileşenler) bulur. Varyans, verinin yayılmasını veya dağılımını ölçer. Verinin farklı özellikleri arasında yüksek varyans, genellikle daha fazla bilgi içerdiği anlamına gelir.\n",
    "İlk temel bileşen (PC1), verinin en yüksek varyansa sahip yönüdür. İkinci temel bileşen (PC2) ise, ilk bileşene dik olan ve ikinci en fazla varyansı gösteren yönüdür ve bu şekilde devam eder.\n",
    "Varyans Oranı:\n",
    "\n",
    "Her temel bileşenin açıklanan varyans oranı, o bileşenin verinin toplam varyansına katkısını gösterir. İlk birkaç temel bileşen genellikle verinin çoğu varyansını yakalar.\n",
    "Bu varyans oranlarını kullanarak, hangi bileşenlerin veri için en anlamlı olduğunu belirleriz. Örneğin, verinin %90'ını açıklayan ilk 3 bileşen seçilebilir ve geri kalan bileşenler atılabilir.\n",
    "Boyut İndirgeme Kararı:\n",
    "\n",
    "PCA uygulandıktan sonra, istediğiniz boyut sayısını belirleyebilirsiniz. Örneğin, n_components=10 seçildiğinde, verinin ilk 10 temel bileşeni seçilir.\n",
    "Eğer verinin çoğu varyansı ilk birkaç bileşende açıklanıyorsa, daha düşük bir boyut sayısı (örneğin, 2 veya 3) seçebilirsiniz ve böylece veriyi daha kompakt hale getirebilirsiniz.\n",
    "Bu karar, explained_variance_ratio_ değerlerine bakarak yapılabilir. Bu oranlar, her bir bileşenin verinin toplam varyansını ne kadar açıkladığını gösterir. İlk birkaç bileşenin toplam açıklanan varyansı genellikle yüksek olur, bu da daha fazla bileşen seçmek yerine daha azını seçmek için bir rehber olabilir.\n"
   ],
   "id": "da41f35ed1454881"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Fit PCA\n",
    "pca = PCA(n_components=5)  # Keep top 5 components\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Get PCA components and feature importance\n",
    "pca_components = pca.components_\n",
    "\n",
    "# Create a DataFrame to associate components with features\n",
    "pca_df = pd.DataFrame(pca_components.T, index=columns, columns=[f\"PC{i+1}\" for i in range(pca.n_components)])\n",
    "\n",
    "# Calculate the absolute contribution of each feature to the principal components\n",
    "importance = pca_df.abs().sum(axis=1)\n",
    "\n",
    "# Sort features by their importance\n",
    "sorted_importance = importance.sort_values(ascending=False)\n",
    "\n",
    "with open(\"PCA_important_features.txt\", \"w\") as f:\n",
    "    f.write(f\"{sorted_importance}\\n\")\n",
    "\n"
   ],
   "id": "a3fcfaa840805b6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4. F-Score Feature Selection",
   "id": "d7b1961c411027ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# F-skoru hesaplama\n",
    "f_scores, p_values = f_classif(X, y)\n",
    "\n",
    "# F-skorlarını ve p-değerlerini iyiden kötüye sıralama\n",
    "sorted_indices = np.argsort(f_scores)[::-1]\n",
    "sorted_f_scores = f_scores[sorted_indices]\n",
    "sorted_p_values = p_values[sorted_indices]\n",
    "sorted_columns = [columns[i] for i in sorted_indices]\n",
    "\n",
    "# Çıktıları dosyaya yazdırma\n",
    "output_file = \"F-score_feature_selection.txt\"\n",
    "with open(output_file, \"w\") as file:\n",
    "    file.write(\"Feature Scores (Sorted):\\n\")\n",
    "    for feature, f_score, p_value in zip(sorted_columns, sorted_f_scores, sorted_p_values):\n",
    "        file.write(f\"Feature: {feature:<20} | F-score: {f_score:.4f} | P-value: {p_value:.4e}\\n\")\n",
    "\n",
    "    # Seçim için bir eşik belirleme (örneğin, F-skoru > 100)\n",
    "    threshold = 100\n",
    "    selected_indices = np.where(f_scores > threshold)[0]\n",
    "    selected_features = [columns[i] for i in selected_indices]\n",
    "\n",
    "    # Seçilen özellikleri yazdırma\n",
    "    file.write(\"\\nSelected Features (F-score > 10):\\n\")\n",
    "    for feature in selected_features:\n",
    "        file.write(f\"- {feature}\\n\")\n",
    "\n",
    "print(f\"Feature selection results have been written to '{output_file}'.\")\n"
   ],
   "id": "12517156c645bf8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "5. Lasso Regression (L1 Regularization):",
   "id": "4ff9efd03d2baa54"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "# Apply Lasso regularization\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Selected features based on non-zero coefficients\n",
    "selected_features = np.where(lasso.coef_ != 0)[0]\n",
    "with open(\"L1_selected_features.txt\", \"w\") as f:\n",
    "    for feature_index in selected_features:\n",
    "        f.write(f\"{columns[feature_index]}\\n\")\n"
   ],
   "id": "f34b64ac1558add5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
